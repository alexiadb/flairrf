{"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"f631ae2d-c169-461c-937a-3446a78abe79","tags":[]},"source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1nTSs6Ilt-8a7zbzELOZHLMIMicNpNRTu\" alt=\"Drawing\"  width=\"35%\"/><center>\n","\n","\n","# <center><strong>Semantic segmentation baseline</strong></center>\n","<br/>\n","\n","<br/><center>This notebook allows you to re-create the baseline results proposed in the **FLAIR-one challenge**.<br/>The code bellow works with the toy dataset (subset) provided in the starting-kit alongside this notebook as well as with the full FLAIR-one dataset accessible after registration to the competition.</center> <br/><br/> \n","    \n","<hr style=\"height:2px;border-width:0;color:red;background-color:red\"> \n","    \n","<center>Before starting this notebook, be sure to read the <font color='red'><b>README.md</b></font> file part of the starting-kit.<br/>It countains information necesary for being able to run this notebook smoothly (requirements and file access).<br/><br/>\n","We also strongly advise you to read the data technical description provided in the <font color='#D7881C'><em>FLAIR-one_dataset.pdf</em></font> file.</center>\n","    \n","\n","<hr style=\"height:2.5px;border-width:0;color:red;background-color:red\"> \n","<br/> <br/> \n","\n","\n","\n","The notebook is composed of two parts :<br/> \n",">(1) <font color='#90c149'><b>data vizualisation</b></font>, allowing you to display some samples and get familiar with the inputs;<br/> \n",">(2) the <font color='#90c149'><b>deep-learning framework baseline</b></font> relying on a U-Net architecture.\n","   \n","\n","<br/><br/><br/><br/>"],"id":"f631ae2d-c169-461c-937a-3446a78abe79"},{"cell_type":"markdown","metadata":{"id":"z-W60WydAVi0","tags":[]},"source":["------  If you are using <font color='#90c149'><b>Colab</b></font>, make sure to <font color='#90c149'>run the following cell</font>.  ------ <br>"],"id":"z-W60WydAVi0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fd6n4D3lBQYK"},"outputs":[],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","os.chdir('/content/gdrive/MyDrive/flair-one-starting-kit/') # set last part according to where the starting-kit is (if linked from shared folder, leave as default, else adjust).\n","\n","!pip install rasterio\n","!pip install pytorch-lightning\n","!pip install segmentation-models-pytorch\n","!pip install albumentations"],"id":"fd6n4D3lBQYK"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"a575f009-b9e4-481c-af32-5f78e82a84c0","tags":[]},"source":["<br/><br/><br/><br/><br/><br/><br/>\n","\n","# <font color='red'>PART-1: Data vizualisation</font>\n","\n","First, let's import relevant functions from the <font color='#D7881C'><em>data_display.py</em></font> file. \n","<br/>"],"id":"a575f009-b9e4-481c-af32-5f78e82a84c0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d403bff2-f610-41d6-b511-9cbfaa07a6e1"},"outputs":[],"source":["from pathlib import Path\n","from py_module.data_display import get_data_paths, display_nomenclature, display_samples, display_all_with_semantic_class, display_all"],"id":"d403bff2-f610-41d6-b511-9cbfaa07a6e1"},{"cell_type":"markdown","metadata":{"id":"88a3b469-b677-4aa2-826e-c00e4476805b","tags":[]},"source":["## <font color='#90c149'>Nomenclatures</font>\n","\n","<br/><hr>\n","\n","Next, we display the semantic land-cover classes used in the FLAIR-one datatset. You will see that <font color='#90c149'>two nomenclatures are available </font> : \n","<ul>\n","    <li>the <strong><font color='#90c149'>full nomenclature</font></strong> corresponds to the semantic classes used by experts in photo-interpretation to label the pixels of the ground-truth images.</li>\n","    <li>the <font color='#90c149'><b>main (baseline) nomenclature</b></font> is a simplified version of the full nomenclature. It regroups (into the class 'other') classes that are either strongly under-represented or irrelevant to this challenge.</li>\n","</ul>        \n","See the associated <font color='#D7881C'><em>FLAIR-one_dataset.pdf</em></font> for additionnal details on these nomenclatures.<br/><br/>\n","\n","<font color='#90c149'>Note:</font> in the data exploration part, we employ the full nomenclature. For the second part related to the challenge baseline, the main nomenclature is used. <br/><hr><br/> "],"id":"88a3b469-b677-4aa2-826e-c00e4476805b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4be159c6-f1d1-4212-84bc-c469dc52d399"},"outputs":[],"source":["display_nomenclature()"],"id":"4be159c6-f1d1-4212-84bc-c469dc52d399"},{"cell_type":"markdown","metadata":{"id":"e147039d-d45e-4ab4-a670-7093c603d7ed","tags":[]},"source":["## <font color='#90c149'>Data display</font>\n","\n","<br/><hr>\n","\n","We start by creating lists containing the paths to the input images (`images`) and supervision masks (`masks`) files of the dataset.<hr><br/>"],"id":"e147039d-d45e-4ab4-a670-7093c603d7ed"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f816639a-7eed-452c-bada-efc4dbf30d5d"},"outputs":[],"source":["path_toy_data = '/content/drive/MyDrive/flair-one-starting-kit/toy_dataset_flair-one'\n","\n","images = sorted(list(get_data_paths(Path(path_toy_data,'train'), 'IMG*.tif')), key=lambda x: int(x.split('_')[-1][:-4]))\n","masks  = sorted(list(get_data_paths(Path(path_toy_data,'train'), 'MSK*.tif')), key=lambda x: int(x.split('_')[-1][:-4]))"],"id":"f816639a-7eed-452c-bada-efc4dbf30d5d"},{"cell_type":"markdown","metadata":{"id":"48cea476-3bd9-4de8-a562-2b325703d3b7","tags":[]},"source":["<br/><hr>\n","\n","Let's display some random samples of IMG-MSK pairs. <font color='#90c149'>Re-run the cell bellow for a different batch.</font><hr><br/>"],"id":"48cea476-3bd9-4de8-a562-2b325703d3b7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1a99d4b7-3f63-4321-bf59-b895ecada184"},"outputs":[],"source":["display_samples(images, masks, nb_samples=1)"],"id":"1a99d4b7-3f63-4321-bf59-b895ecada184"},{"cell_type":"markdown","metadata":{"id":"f8e4fe44-5f9c-4e69-881b-e24b64c205c8","tags":[]},"source":["<br/><hr>\n","\n","Next let's have a closer look at some specific semantic class.<br/> By setting `semantic_class` to a class number (*e.g.*, `semantic_class`=1 for building or `semantic_class`=5 for water) we can visualize the images containing pixels of this specific class. (the full nomenclature is be used.)<br/>\n","<font color='#90c149'>Note:</font> for Colab users, this can take some time. <hr><br/>"],"id":"f8e4fe44-5f9c-4e69-881b-e24b64c205c8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ce3fcb7-8c9b-4207-bdf5-1227ba6600f0"},"outputs":[],"source":["display_all_with_semantic_class(images, masks, semantic_class=1)"],"id":"9ce3fcb7-8c9b-4207-bdf5-1227ba6600f0"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"da00bce4-c1da-4843-9705-e573a74a7dd9","tags":[]},"source":["<br/><hr> \n","\n","If working with the toy dataset, we can directly display all images!<br/>\n","<font color='#90c149'>Note:</font> for Colab users, this can take some time. <hr><br/>"],"id":"da00bce4-c1da-4843-9705-e573a74a7dd9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3b7d91f-3409-471a-b40c-8ca98be09a3f"},"outputs":[],"source":["display_all(images, masks)"],"id":"a3b7d91f-3409-471a-b40c-8ca98be09a3f"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"a0bc22ea-1ef0-4f21-aa8c-171807d11362","tags":[]},"source":["<br/><br/><br/><br/><br/><br/><br/><br/><br/>\n","\n","# <font color='red'>PART-2: Baseline </font>\n","\n","<br/><hr>\n","\n","In this second part, we use the toy dataset to train a model similar to the FLAIR-one baseline provided with the challenge.<br/> \n","<font color='#90c149'>Note:</font> the presented pipeline can also be applied to the full dataset.\n","\n","First, let's check if GPU ressources are available in our execution environment. If not, and your are running this notebook in a google colab, select Runtime -> Change runtime type -> select GPU.  If not, and you are running this notebook locally, make sure to set `accelerator = 'cpu'` in the parameters.\n","<hr><br/>"],"id":"a0bc22ea-1ef0-4f21-aa8c-171807d11362"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5b1f9ae-bd82-40e6-94ae-3ce7e8376975"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0: print('No GPU found.')\n","else: print(gpu_info)"],"id":"e5b1f9ae-bd82-40e6-94ae-3ce7e8376975"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"e368bf75-b5aa-43e8-80ef-2b03b74e7020","tags":[]},"source":["<br/><hr>\n","\n","The cell bellow imports the required libraries, classes and functions, including those provided in the <font color='#D7881C'><em>py-module</em></font> folder provided with this starting-kit. If you are running this notebook on a local environment, make sure all necessary libraries are installed (refer to the <font color='red'>README.md</font> file).\n","\n","This baseline relies on <font color='#90c149'><em>pytorch-lightning</em></font>, a high-level python framework built on top of Pytorch. It allows multi-GPU training, significantly speeding-up computation of the baseline on the full FLAIR-one dataset. It is however also possible to train on a single GPU as we demonstrate in this notebook.\n","\n","In this notebook, we also take advantage of the <font color='#90c149'><em>segmentation-models-pytorch</em></font> library, which provides a variery of different pre-trained segmentation models (*e.g.*, U-Net, PSPNet,...).\n","<hr><br/>"],"id":"e368bf75-b5aa-43e8-80ef-2b03b74e7020"},{"cell_type":"code","source":["import os\n","import logging\n","import glob \n","\n","import numpy as np\n","import pandas as pd\n"],"metadata":{"id":"O-SIwmnOrhZZ"},"id":"O-SIwmnOrhZZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/bin/python3\n","#This file transforms a tiff image to a dataframe\n","\n","\n","def set_environment(global_vars: dict) -> int:\n","\t''' Sets all the variables for the environments into a params variables\n","\n","\tparams:\n","\t-------\n","\t\tglobal_vars : \tdict : all the variables will be stored in this dictionnary\n","\n","\treturns:\n","\t\t0 \t\t\t: standard linux return's value without an error\n","\n","\t'''\n","\n","\tglobal_vars['inputdir']= '/content/drive/MyDrive/flair-one-starting-kit/toy_dataset_flair-one'\n","\tglobal_vars['outputdir']= '/content/drive/MyDrive/flair-ign-lde_rf/random-forest/dfs'\n","\tlogging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG)\n","\tglobal_vars['logger'] = logging.getLogger(__name__)\n","\tglobal_vars['cal_ndvi'] = True\n","\n","\t\n","\treturn 0\n","\n","\n","\n","desc = \"\"\"\n","\tStarting Transformation function from Tiff to DataFrame\n","\tinputs : /inputdir : full paths of the input tiff files\n","\t \t/outputdir : directory of the outputs files   \n","\"\"\"\n","\n","def output_file(global_var: dict, input_file: str) -> str:\n","\t'''function to write to output filename of the pandas dataframe from an input file\n","\t'''\n","\n","\tifilename = os.path.basename(input_file).split('.')[0]\n","\toutputdir = global_var['outputdir']\n","\n","\tofilename =  os.path.join(outputdir,\"{0}.csv\".format(ifilename))\n","\n","\treturn ofilename\n","\n","def tiff2df_3D(global_var : dict, input_file: str) -> pd.DataFrame : \n","\t'''Reads a tiff file full path, and generates a pandasDataFrame from it\n","\t\tThe functions uses the libraries gdal (osgeo)\n","\t\thttps://pcjericks.github.io/py-gdalogr-cookbook/raster_layers.html\n","\t'''\n","\n","\tlogger = global_var['logger']\n","\tlogger.info('reading input file {0}'.format(input_file))\n","\n","\tnp_array = gdal_array.LoadFile(input_file)\n","\n","\tz_dim,x_dim,y_dim = np.shape(np_array)\n","\n","\tvalues = np.transpose(np_array.reshape(z_dim,-1)) / 255.\n","\ti_rows = np.array( [[j for i in range(x_dim)] for j in range(y_dim)]).flatten()\n","\tj_cols = np.array(x_dim * [j for j in range(y_dim)])\n","\n","\tdf = pd.DataFrame(data = values, columns=['R','G','B', 'N', 'H'])\n","\n","\tdf['i_rows'] = i_rows\n","\tdf['j_cols'] = j_cols\n","\n","\treturn df\n","\n","def tiff2df_1D(global_var : dict, input_file: str) -> pd.DataFrame : \n","\t'''Reads a tiff file full path, and generates a pandasDataFrame from it\n","\t\tThe functions uses the libraries gdal (osgeo)\n","\t\thttps://pcjericks.github.io/py-gdalogr-cookbook/raster_layers.html\n","\t\tof 2 dimensional array (for class)\n","\t'''\n","\n","\tlogger = global_var['logger']\n","\tlogger.info('reading input file {0}'.format(input_file))\n","\n","\tnp_array = gdal_array.LoadFile(input_file)\n","\n","\tx_dim,y_dim = np.shape(np_array)\n","\t\n","\tvalues = np_array.reshape(-1) \n","\ti_rows = np.array( [[j for i in range(x_dim)] for j in range(y_dim)]).flatten()\n","\tj_cols = np.array(x_dim * [j for j in range(y_dim)])\n","\n","\tdf = pd.DataFrame(data = values, columns=['C'])\n","\n","\tdf['i_rows'] = i_rows\n","\tdf['j_cols'] = j_cols\n","\n","\treturn df\n","\n","def save(global_var : dict, idf : pd.DataFrame, input_file : str) -> int : \n","\t'''save dataframe to a file in the output directory\n","\t'''\n","\tofilename = output_file(global_var, input_file) + '.gzip'\n","\n","\tlogger.info('saving to file: {0}'.format(ofilename))\n","\n","\tidf.to_csv(path_or_buf=ofilename, \n","\t\tindex=False, \n","\t\tcompression={'method': 'gzip', 'compresslevel': 6}) \n","\n","\treturn 0\n","\n","def ndvi(global_var: dict, idf: pd.DataFrame) -> pd.DataFrame: \n","\t'''function to calculate ndvi index from a dataframe\n","\t'''\n","\tidf['ndvi'] = (idf['R']-idf['G']) / (idf['R']+idf['G'])\n","\n","\treturn idf\n","\n","def do_msk(global_var : dict) -> int:\n","\t'''Performs mask only it is assumed a 1D array in input\n","\t'''\n","\t#read file structure\n","\n","\tinputdir = global_var['inputdir']\n","\tinput_files = glob.glob(os.path.join(inputdir,'**','MSK_*.tif'), recursive=True)\n","\n","\tfor input_file in input_files:\n","\t\tdf=tiff2df_1D(global_var, input_file)\t\t\n","\t\tsave(global_var, df,input_file)\n","\n","\treturn 0\n","\n","def do_train(global_var : dict) -> int:\n","\t'''Performs train only \n","\t'''\n","\tlogger = global_var['logger']\n","\tinputdir = global_var['inputdir']\n","\tcal_ndvi = global_var['cal_ndvi']\n","\n","\n","\t#read file structure\n","\n","\tinput_files = glob.glob(os.path.join(inputdir,'**','IMG_*.tif'), recursive=True)\n","\n","\tfor input_file in input_files:\n","\t\tdf=tiff2df_3D(global_var, input_file)\n","\n","\t\tif cal_ndvi:\n","\t\t\tdf = ndvi(global_var, df)\t\n","\t\t\n","\t\tsave(global_var, df,input_file)\n","\n","\treturn 0\n","\n","if __name__ == '__main__':\n","\t\n","\tglobal_var = {}\n","\tset_environment(global_var)\n","\tlogger = global_var['logger']\n","\tlogger.info(desc)\n","\n","\tlogger.info('global_var {0}'.format(global_var))\n","\n","\tlogger.info('transforming training ... ')\n","\tdo_train(global_var)\n","\n","\tlogger.info('transforming mask ...')\n","\tdo_msk(global_var)\n","\n"],"metadata":{"id":"_5T_oWLKqMMT"},"id":"_5T_oWLKqMMT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/bin/python\n","#This file transforms a tiff image to a dataframe\n","\n","import os\n","import logging\n","import glob \n","import pandas as pd\n","from osgeo import gdal_array\n","import numpy as np\n","\n","def set_environment(global_vars: dict) -> int:\n","\t''' Sets all the variables for the environments into a params variables\n","\n","\tparams:\n","\t-------\n","\t\tglobal_vars : \tdict : all the variables will be stored in this dictionnary\n","\n","\treturns:\n","\t\t0 \t\t\t: standard linux return's value without an error\n","\n","\t'''\n","\n","\tglobal_vars['inputdir']='/inputdir'\n","\tglobal_vars['outputdir']='/outputdir'\n","\tlogging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG)\n","\tglobal_vars['logger'] = logging.getLogger(__name__)\n","\tglobal_vars['cal_ndvi'] = True\n","\n","\t\n","\treturn 0\n","\n","\n","\n","desc = \"\"\"\n","\tadd the classes ground true label to the  csv dataframe  \n","\tinputs : /inputdir : full paths of the msk and img csv files\n","\t \t/outputdir : directory of the outputs csv files   \n","\"\"\"\n","\n","def do_combine(globar_var: dict) -> int:\n","\t'''creates the training set from an img csv and a mask csv\n","\t'''\n","\n","\tinputdir = global_var['inputdir']\n","\toutputdir = global_var['outputdir']\n","\tlogger = global_var['logger']\n","\n","\tmsk_files = glob.glob(os.path.join(inputdir,'**','MSK_*.csv.gzip'), recursive=True)\n","\n","\tfor msk_file in msk_files: \n","\t\timg_file = msk_file.replace('MSK','IMG')\n","\n","\t\tif os.path.exists(img_file):\n","\t\t\tlogger.info('combining {0}'.format(msk_file))\n","\n","\t\t\tdf_train = pd.read_csv(img_file, compression={'method': 'gzip', 'compresslevel': 6})\n","\t\t\tdf_msk = pd.read_csv(msk_file, compression={'method': 'gzip', 'compresslevel': 6})\n","\t\t\tdf_train['C'] = df_msk['C']\n","\t\t\tofilename=os.path.join(outputdir,os.path.basename(img_file))\n","\n","\t\t\tdf_train.to_csv(path_or_buf=ofilename, \n","\t\t\t\t\tindex=False, \n","\t\t\t\t\tcompression={'method': 'gzip', 'compresslevel': 6})\n","\t\telse:\n","\t\t\tlogger.warning('corresponding img of {0} not found'.format(msk_file))\n","\n","\treturn 0\n","\n","\n","if __name__ == '__main__':\n","\t\n","\tglobal_var = {}\n","\tset_environment(global_var)\n","\tlogger = global_var['logger']\n","\tlogger.info(desc)\n","\n","\tlogger.info('global_var {0}'.format(global_var))\n","\n","\tlogger.info('combining training ... ')\n","\tdo_combine(global_var)\n","\n"],"metadata":{"id":"UGAJOlnFt8q8"},"id":"UGAJOlnFt8q8","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0e9c1462-91a7-4d3c-b02b-1161380ffcf8"},"outputs":[],"source":["#general \n","import os\n","os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION']='python'\n","import numpy as np\n","import json\n","import random\n","from pathlib import Path \n","\n","#deep learning\n","import torch\n","import torch.nn as nn\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n","from pytorch_lightning.callbacks.progress.tqdm_progress import TQDMProgressBar\n","from pytorch_lightning import Trainer, seed_everything\n","try:\n","  from pytorch_lightning.utilities.distributed import rank_zero_only\n","except ImportError:\n","  from pytorch_lightning.utilities.rank_zero import rank_zero_only  \n","\n","import albumentations as A\n","\n","#flair-one baseline modules \n","from py_module.utils import load_data, subset_debug\n","from py_module.datamodule import OCS_DataModule\n","from py_module.model import SMP_Unet_meta\n","from py_module.task_module import SegmentationTask\n","from py_module.writer import PredictionWriter"],"id":"0e9c1462-91a7-4d3c-b02b-1161380ffcf8"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"3ccd2527-a222-4ed1-a4e8-899595d0cc5d","tags":[]},"source":["## <font color='#90c149'>Task and parameters</font>\n","\n","<br/><hr>\n","\n","The toy dataset is composed of $200$ training patches of $512x512$ with corresponding semantic masks, and $50$ test patches. The full dataset contains $61,712$ training patches and $15,700$ testing patches.<br/><br/>\n","\n","The next cell defines <font color='#90c149'>the paths and hyper-parameters</font>. We recommand starting with the given default values and test if everything is working (check the <font color='#D7881C'><em>FLAIR-one_dataset.pdf</em></font> file for the baseline hyper-parameters). \n","\n","\n","The `use_weight` variable enables class-weights during the Cross Entropy loss computation. As the 'other' semantic class is ill-defined, it's weight is set to $0$ as the default strategy.\n","\n","The `use_metadata` variable enables or disables the use of metadata. If set to `True`, encoded metadata will be added to the architecture's encoder output.\n","\n","The `use_augmentation` variable enables geometric and radiometric image data augmentations during training. \n","\n","See the <font color='#D7881C'><em>FLAIR-one_dataset.pdf</em></font> file for details about the metadata and image data augmentation strategies and the <font color='#D7881C'><em>utils.py</em></font>, <font color='#D7881C'><em>dataset.py</em></font>  and <font color='#D7881C'><em>model.py</em></font> files for implementation. \n","\n","<hr><br/>"],"id":"3ccd2527-a222-4ed1-a4e8-899595d0cc5d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcc904b6-e26e-452b-913b-401394512b95"},"outputs":[],"source":["##############################################################################################\n","# paths and naming\n","path_data = \"/work/users/flair-ign/starting-kit/toy_dataset_flair-one\" # toy (or full) dataset folder\n","path_metadata_file = \"/work/users/flair-ign/starting-kit/metadata/flair-one_TOY_metadata.json\" # json file containing the metadata\n","\n","out_folder = \"/work/users/flair-ign/output/models_output/\" # output directory for logs and predictions.\n","out_model_name = \"flair-one-baseline_argu\" # to keep track\n","##############################################################################################\n","\n","##############################################################################################\n","# tasking\n","use_weights = True \n","class_weights = [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0]\n","\n","use_metadata = False\n","use_augmentation = False\n","##############################################################################################\n","\n","##############################################################################################\n","# training hyper-parameters\n","batch_size = 2\n","learning_rate = 0.02\n","num_epochs = 3\n","##############################################################################################\n","\n","##############################################################################################\n","# computational ressources\n","accelerator = 'gpu' # set to 'cpu' if GPU not available\n","gpus_per_node = 1 # set to 1 if mono-GPU\n","num_nodes = 1 # set to 1 if mono-GPU\n","strategy = None # Put this parameter to None if train on only one GPU or on CPUs. If multiple GPU, set to 'ddp'\n","num_workers = 4\n","##############################################################################################\n","\n","##############################################################################################\n","# display\n","enable_progress_bar = True\n","progress_rate = 10 #tqdm update rate during training \n","##############################################################################################"],"id":"bcc904b6-e26e-452b-913b-401394512b95"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"c7933736-da55-4494-ae1b-d4ed0248e684","tags":[]},"source":["## <font color='#90c149'>Dataloaders</font>\n","\n","<br/><hr>\n","\n","The following cell loads the data into a pytorch-lighning DataModule (`OCS_Datamodule`). If `use_metadata = True` and `use_augmentation = True`, they are also integrated in the DataModule. \n","\n","We fix the global seed (python random, torch, numpy) with `seed_eveything`.\n","\n","`@rank_zero_only` is needed if logging and callbacks are used with multi-GPU processing, preventing synchronization from concurrent processes.\n","\n","<hr><br/>"],"id":"c7933736-da55-4494-ae1b-d4ed0248e684"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b057be80-7aa5-4e1f-b7c6-77594906359e"},"outputs":[],"source":["out_dir = Path(out_folder, out_model_name)\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","seed_everything(2022, workers=True)\n","\n","@rank_zero_only\n","def step_loading(path_data, path_metadata_file: str, use_metadata: bool) -> dict:\n","    print('+'+'-'*29+'+', '   LOADING DATA   ', '+'+'-'*29+'+')\n","    train, val, test = load_data(path_data, path_metadata_file, use_metadata=use_metadata)\n","    return train, val, test\n","\n","\n","@rank_zero_only\n","def print_recap():\n","    print('\\n+'+'='*80+'+',f\"{'Model name: '+out_model_name : ^80}\", '+'+'='*80+'+', f\"{'[---TASKING---]'}\", sep='\\n')\n","    for info, val in zip([\"use weights\", \"use metadata\", \"use augmentation\"], [use_weights, use_metadata, use_augmentation]): print(f\"- {info:25s}: {'':3s}{val}\")\n","    print('\\n+'+'-'*80+'+', f\"{'[---DATA SPLIT---]'}\", sep='\\n')\n","    for split_name, d in zip([\"train\", \"val\", \"test\"], [dict_train, dict_val, dict_test]): print(f\"- {split_name:25s}: {'':3s}{len(d['IMG'])} samples\")\n","    print('\\n+'+'-'*80+'+', f\"{'[---HYPER-PARAMETERS---]'}\", sep='\\n')\n","    for info, val in zip([\"batch size\", \"learning rate\", \"epochs\", \"nodes\", \"GPU per nodes\", \"accelerator\", \"workers\"], [batch_size, learning_rate, num_epochs, num_nodes, gpus_per_node, accelerator, num_workers]): print(f\"- {info:25s}: {'':3s}{val}\")        \n","    print('\\n+'+'-'*80+'+', '\\n')\n","\n","dict_train, dict_val, dict_test = step_loading(path_data, path_metadata_file, use_metadata=use_metadata)  \n","print_recap()\n","\n","\n","\n","if use_augmentation == True:\n","    transform_set = A.Compose([A.VerticalFlip(p=0.5),\n","                               A.HorizontalFlip(p=0.5),\n","                               A.RandomRotate90(p=0.5)])\n","else:\n","    transform_set = None\n","\n","dm = OCS_DataModule(\n","    dict_train=dict_train,\n","    dict_val=dict_val,\n","    dict_test=dict_test,\n","    batch_size=batch_size,\n","    num_workers=num_workers,\n","    drop_last=True,\n","    num_classes=13,\n","    num_channels=5,\n","    use_metadata=use_metadata,\n","    use_augmentations=transform_set)"],"id":"b057be80-7aa5-4e1f-b7c6-77594906359e"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"0c4c744e-9ff1-4acb-8f52-4c063b229915","tags":[]},"source":["## <font color='#90c149'>Learning setup</font>\n","\n","<br/><hr>\n","\n","Next, we define our <font color='#90c149'>model, criterion, optimizer and scheduler</font>.\n","\n","The model `SMP_Unet_meta` uses a ResNet34 encoder and U-Net decoder implementated in the <em>segmentation-models-pytorch</em> library. If `use_metadata = True`, it adds a custom Multi-layer Perceptron, encoding the metadata.\n","\n","As criterion, we use the torch implementation of `CrossEntropyLoss`, and as optimizer the Stochastic Gradient Descend (`SGD`).\n","\n","The pytorch-lighning module `SegmentationTask` organizes and manages the different loops and steps (e.g., training, validation), otherwise manually implemented using torch.\n","\n","Finally we define `callbacks` (save model checkpoints, stop if learning is stuck with a patience threshold and display progress) as well as a `logger` (tensorboard logs).\n","\n","<hr><br/>"],"id":"0c4c744e-9ff1-4acb-8f52-4c063b229915"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"f2069ada-8d0a-49e6-82af-6dfb477cdfe7","tags":[]},"source":["### Model\n","<font color='#90c149'>Note:</font> the next cell will trigger the download of ResNet34 (default for U-Net architecture in pytorch-lightning) with pre-trained weights."],"id":"f2069ada-8d0a-49e6-82af-6dfb477cdfe7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cdc448b-40fa-4e3f-bfe4-7c506ba60820"},"outputs":[],"source":["model = SMP_Unet_meta(n_channels=5, n_classes=13, use_metadata=use_metadata)"],"id":"0cdc448b-40fa-4e3f-bfe4-7c506ba60820"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"a7f8c2f2-8241-448e-a534-accbae1b6bf2","tags":[]},"source":["### Loss"],"id":"a7f8c2f2-8241-448e-a534-accbae1b6bf2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fedcbc1-9da5-4167-9407-9c4dc427cdbb"},"outputs":[],"source":["if use_weights == True:\n","    with torch.no_grad():\n","        class_weights = torch.FloatTensor(class_weights)\n","    criterion = nn.CrossEntropyLoss(weight=class_weights)\n","else:\n","    criterion = nn.CrossEntropyLoss()"],"id":"4fedcbc1-9da5-4167-9407-9c4dc427cdbb"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"2c65e30a-91ce-463b-9168-8b09c827f7a6","tags":[]},"source":["### Optimizer"],"id":"2c65e30a-91ce-463b-9168-8b09c827f7a6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c86049ef-0f11-4654-85d9-2797a86e33c7"},"outputs":[],"source":["optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"id":"c86049ef-0f11-4654-85d9-2797a86e33c7"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"0bbcef33-579c-4b33-b43b-dcfdfb5689a5","tags":[]},"source":["### Scheduler"],"id":"0bbcef33-579c-4b33-b43b-dcfdfb5689a5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d66a33fb-44f0-462c-8735-ddefa6d09ee3"},"outputs":[],"source":["scheduler = ReduceLROnPlateau(\n","    optimizer=optimizer,\n","    mode=\"min\",\n","    factor=0.5,\n","    patience=10,\n","    cooldown=4,\n","    min_lr=1e-7,\n",")"],"id":"d66a33fb-44f0-462c-8735-ddefa6d09ee3"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"70ee60d0-823d-46fd-957c-e9341fa3aa25","tags":[]},"source":["### Pytorch lightning module"],"id":"70ee60d0-823d-46fd-957c-e9341fa3aa25"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4d2f5dc2-4163-490a-8e41-f8742ba5efc9"},"outputs":[],"source":["seg_module = SegmentationTask(\n","    model=model,\n","    num_classes=dm.num_classes,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    use_metadata=use_metadata\n",")"],"id":"4d2f5dc2-4163-490a-8e41-f8742ba5efc9"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"0b9d2cbb-f1ac-4f8d-88f5-f347c0a9e6e8","tags":[]},"source":["### Callbacks"],"id":"0b9d2cbb-f1ac-4f8d-88f5-f347c0a9e6e8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2041fd30-c39e-49ee-9cb7-f00774ffb1b2"},"outputs":[],"source":["ckpt_callback = ModelCheckpoint(\n","    monitor=\"val_loss\",\n","    dirpath=os.path.join(out_dir,\"checkpoints\"),\n","    filename=\"ckpt-{epoch:02d}-{val_loss:.2f}\"+'_'+out_model_name,\n","    save_top_k=1,\n","    mode=\"min\",\n","    save_weights_only=True, # can be changed accordingly\n",")\n","\n","early_stop_callback = EarlyStopping(\n","    monitor=\"val_loss\",\n","    min_delta=0.00,\n","    patience=30, # if no improvement after 30 epoch, stop learning. \n","    mode=\"min\",\n",")\n","\n","prog_rate = TQDMProgressBar(refresh_rate=progress_rate)\n","\n","callbacks = [\n","    ckpt_callback, \n","    early_stop_callback,\n","    prog_rate,\n","]"],"id":"2041fd30-c39e-49ee-9cb7-f00774ffb1b2"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"d63d51b5-249d-4838-bbb6-dca8ba997ff9","tags":[]},"source":["### Loggers"],"id":"d63d51b5-249d-4838-bbb6-dca8ba997ff9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c56fda75-3b52-4a4e-874f-083c06b446e6"},"outputs":[],"source":["logger = TensorBoardLogger(\n","    save_dir=out_dir,\n","    name=Path(\"tensorboard_logs\"+'_'+out_model_name).as_posix()\n",")\n","\n","loggers = [\n","    logger\n","]"],"id":"c56fda75-3b52-4a4e-874f-083c06b446e6"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"7248b8bc-638e-4085-bd83-7baa94cd094b","tags":[]},"source":["## <font color='#90c149'>Launch the training</font>\n","\n","<br/><hr>\n","\n","Defining a `Trainer` allows for to automate tasks, such as enabling/disabling grads, running the dataloaders or invoking the callbacks when needed.\n","<hr><br/>"],"id":"7248b8bc-638e-4085-bd83-7baa94cd094b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"19ceb0de-9a75-43e9-be60-a7a4a1640d7b"},"outputs":[],"source":["#### instanciation of  Trainer\n","trainer = Trainer(\n","    accelerator=accelerator,\n","    devices=gpus_per_node,\n","    strategy=strategy,\n","    num_nodes=num_nodes,\n","    max_epochs=num_epochs,\n","    num_sanity_val_steps=0,\n","    callbacks = callbacks,\n","    logger=loggers,\n","    enable_progress_bar = enable_progress_bar,\n",")"],"id":"19ceb0de-9a75-43e9-be60-a7a4a1640d7b"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"cFE9DTm3A8df","tags":[]},"source":["<br/><hr>\n","\n","<font color='#90c149'>Let's launch the training.</font>\n","<br/><hr>"],"id":"cFE9DTm3A8df"},{"cell_type":"code","execution_count":null,"id":"92beebec","metadata":{"id":"92beebec"},"outputs":[],"source":["trainer.fit(seg_module, datamodule=dm)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"48978bf6-4d34-4308-ad8a-105efb45ea79","tags":[]},"source":["## <font color='#90c149'>Check metrics on the validation dataset</font>\n","\n","<br/><hr> \n","\n","To give an idea on the training results, we call validate on the trainer to print some metrics. <hr><br/>"],"id":"48978bf6-4d34-4308-ad8a-105efb45ea79"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8d96d9d7-da3c-4266-9f4b-712f4c923aa6"},"outputs":[],"source":["trainer.validate(seg_module, datamodule=dm)"],"id":"8d96d9d7-da3c-4266-9f4b-712f4c923aa6"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"a3984d2d-7ed0-4d9b-8d47-47fce2d88987","tags":[]},"source":["## <font color='#90c149'>Inference and predictions export</font>\n","\n","<br/><hr>\n","\n","For inference, we define a new callback, `PredictionWriter`, which is used to export the predictions on the test dataset.<br/><br/>\n","<font color='#90c149'>Note:</font> the callback exports the files with the mandotary formatting of outputs (files named <font color='red'><b> PRED_{ID].tif</b></font>, with datatype <font color='red'><b>uint8</b></font> and <font color='red'><b>LZW</b></font> compression), using Pillow.\n","Check the <font color='#D7881C'><em>writer.py</em></font> file for details.<br/><br/>\n","\n","We instantiate a new `Trainer` with this newly defined callback and call predict.\n","<hr><br/>"],"id":"a3984d2d-7ed0-4d9b-8d47-47fce2d88987"},{"cell_type":"code","execution_count":null,"metadata":{"id":"329190c2-040d-4a77-a27d-3239286e01e4"},"outputs":[],"source":["writer_callback = PredictionWriter(        \n","    output_dir=os.path.join(out_dir, \"predictions\"+\"_\"+out_model_name),\n","    write_interval=\"batch\",\n",")\n","\n","#### instanciation of prediction Trainer\n","trainer = Trainer(\n","    accelerator=accelerator,\n","    devices=gpus_per_node,\n","    strategy=strategy,\n","    num_nodes=num_nodes,\n","    callbacks = [writer_callback],\n","    enable_progress_bar = enable_progress_bar,\n",")"],"id":"329190c2-040d-4a77-a27d-3239286e01e4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e36febba-49d1-410f-a60f-1fba724e9f76"},"outputs":[],"source":["trainer.predict(seg_module, datamodule=dm)\n","\n","@rank_zero_only\n","def print_finish():\n","    print('--  [FINISHED.]  --', f'output dir : {out_dir}', sep='\\n')\n","print_finish()"],"id":"e36febba-49d1-410f-a60f-1fba724e9f76"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"09b7e1da-f928-4b73-8014-a5ed49fd38ea","tags":[]},"source":["## <font color='#90c149'>Visual checking of predictions</font>\n","\n","<br/><hr>\n","\n","<font color='#90c149'>For the test set, obviously, you do not have access to the masks.</font> Nevertheless, we can visually display some predictions alongside the RGB images.<br/><br/>\n","\n","First, we create lists containing the paths to the test RGB images (`images_test`) as well as the predicted semantic segmentation masks (`predictions`).<br/><br/>\n","\n","\n","\n","We then display some random couples of predictions together with their corresponding aerial RGB images.<br/><br/>\n","\n","<font color='#90c149'><em>Note 1</em></font>: if you are using the toy dataset, don't expect accurate predictions. A set of $200$ training samples will give limited results.<br/> \n","<font color='#90c149'><em>Note 2</em></font>: rasterio will yield a <em>NotGeoreferencedWarning</em> regarding the predictions files. This is normal as the prediction files have been written without any geographical information, which is expected by rasterio. This kind of information is not important for assessing the model outputs, so we can just omit the warning.\n","<hr><br/>"],"id":"09b7e1da-f928-4b73-8014-a5ed49fd38ea"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1e31b62-590a-4a61-9b95-4be828159cff"},"outputs":[],"source":["from py_module.data_display import display_predictions, get_data_paths\n","\n","images_test = sorted(list(get_data_paths(Path(path_data,'test'), 'IMG*.tif')), key=lambda x: int(x.split('_')[-1][:-4]))\n","predictions = sorted(list(get_data_paths(Path(out_dir, \"predictions\"+'_'+out_model_name), 'PRED*.tif')), key=lambda x: int(x.split('_')[-1][:-4]))"],"id":"a1e31b62-590a-4a61-9b95-4be828159cff"},{"cell_type":"code","execution_count":null,"metadata":{"id":"520c9ed4-6ff2-4d07-a15b-86d039e8f2dc"},"outputs":[],"source":["display_predictions(images_test, predictions, nb_samples=2)"],"id":"520c9ed4-6ff2-4d07-a15b-86d039e8f2dc"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"84bb236d-2c92-4140-88af-6a67d74e9b05","tags":[]},"source":["## <font color='#90c149'>Metric calculation: mIoU</font>\n","\n","<br/><hr>\n","\n","As mentioned before, the masks of the test set are not available. However, the following cell describes the code that is used to calculate the metric used over the test set and to consequently rank the best models. Again, the toy dataset contains $50$ test pastches, while the full FLAIR-one dataset contains $15,700$ test patches.<br/><br/>\n","\n","The calculation of the mean Intersection-over-Union (`mIou`) is based on the confusion matrix $C$, which is determined for each test patch. The confusion matrices are subsequently summed providing the confusion matrix describing the test set. Per-class IoU, defined as the ratio between true positives divided by the sum of false positives, false negatives and true positives is calculated from the summed confusion matrix as follows: <br/><br/>\n","    $$\n","    IoU_i = \\frac{C_{i,i}}\n","    {C_{i,i} + \\sum_{j \\neq i}\\left(C_{i,j} + C_{j,i} \\right)} = \\frac{TP}{TP+FP+FN}\n","    $$\n","<br>\n","The final `mIou` is then the average of the per-class IoUs. \n","\n","\n","<font color='#90c149'><em>Note:</em></font> as the <font color='#90c149'><em>'other'</em></font> class is <font color='#90c149'>not well defined (void)</font>, its IoU is <font color='#90c149'>removed</font> and therefore does not contribute to the calculation of the `mIou`. In other words,  the remaining per-class IoUs (all except 'other') are averaged by 12 and not 13 to obtain the final `mIou`.</font>\n","\n","<hr><br/>"],"id":"84bb236d-2c92-4140-88af-6a67d74e9b05"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cde8904-2b0d-4273-9619-42108e3a80c7"},"outputs":[],"source":["import re\n","import numpy as np\n","from PIL import Image\n","from sklearn.metrics import confusion_matrix\n","\n","\n","def generate_miou(path_truth: str, path_pred: str) -> list:\n","  \n","    #################################################################################################\n","    def get_data_paths (path, filter):\n","        for path in Path(path).rglob(filter):\n","             yield path.resolve().as_posix()  \n","                \n","    def calc_miou(cm_array):\n","        m = np.nan\n","        with np.errstate(divide='ignore', invalid='ignore'):\n","            ious = np.diag(cm_array) / (cm_array.sum(0) + cm_array.sum(1) - np.diag(cm_array))\n","        m = np.nansum(ious[:-1]) / (np.logical_not(np.isnan(ious[:-1]))).sum()\n","        return m.astype(float), ious[:-1]      \n","\n","    #################################################################################################\n","                       \n","    truth_images = sorted(list(get_data_paths(Path(path_truth), 'MSK*.tif')), key=lambda x: int(x.split('_')[-1][:-4]))\n","    preds_images  = sorted(list(get_data_paths(Path(path_pred), 'PRED*.tif')), key=lambda x: int(x.split('_')[-1][:-4]))\n","    if len(truth_images) != len(preds_images): \n","        print('[WARNING !] mismatch number of predictions and test files.')\n","    if truth_images[0][-10:-4] != preds_images[0][-10:-4] or truth_images[-1][-10:-4] != preds_images[-1][-10:-4]: \n","        print('[WARNING !] unsorted images and masks found ! Please check filenames.') \n","        \n","    patch_confusion_matrices = []\n","\n","    for u in range(len(truth_images)):\n","        target = np.array(Image.open(truth_images[u]))-1 # -1 as model predictions start at 0 and turth at 1.\n","        target[target>12]=12  ### remapping masks to reduced baseline nomenclature.\n","        preds = np.array(Image.open(preds_images[u]))         \n","        patch_confusion_matrices.append(confusion_matrix(target.flatten(), preds.flatten(), labels=list(range(13))))\n","\n","    sum_confmat = np.sum(patch_confusion_matrices, axis=0)\n","    mIou, ious = calc_miou(sum_confmat) \n","\n","    return mIou, ious\n","\n","\n","#if name == \"__main__\":  \n","#    truth_msk = './reference/\n","#    pred_msk  = './predictions/'\n","#    mIou = generate_miou(truth_images, truth_msk)"],"id":"9cde8904-2b0d-4273-9619-42108e3a80c7"},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"03bd09e5-284a-4f0d-b297-111b6bd87bb7","tags":[]},"source":["<br/><br/><br/><br/>\n","\n","### <center><strong>For any feedback, request, suggestion or simply to say hi, we are reachable at : ai-challenge@ign.fr !</strong></center>\n","<br/>\n","<font size=2.5> <b>@IGN, Nov. 2022</b></font>\n","<img src=\"https://drive.google.com/uc?export=view&id=14clxUsTGj7i6oXt6q9FQeaxzjIi3biI2\" alt=\"Drawing\"  width=\"100%\"/>"],"id":"03bd09e5-284a-4f0d-b297-111b6bd87bb7"}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.3 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":5}